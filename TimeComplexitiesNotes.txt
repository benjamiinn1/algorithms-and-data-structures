An algorithm is O(f(n)) if the number of simple operations the computer has to do is eventually less than a constant times f(n), as n increases

linear (f(n) = n) (ok)
quadratic (f(n) = n^2) (usually pretty bad)
constant (f(n) = 1) (usually the best)

In bigO notation, constants do not matter (when dealing with huge numbers constants mean very little)
O(5) > O(1)
O(2n) > O(n)
O(10n^2) > O(n^2)

In bigO notation, we only care about the largest term.
O(n + 1) > O(n)
O(3n^2 + 2n + 1) > O(n^2)

USUALLY THESE ARE TRUE:
1. variable assignment and arithmetic operations are constant
2. accessing elements in an array are constant.
3. looping increases the complexity for length of n.